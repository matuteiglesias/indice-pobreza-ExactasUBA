{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2020\n",
    "endyr = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n",
    "cpi = pd.read_csv('./../data/info/indice_precios.csv', index_col=0)\n",
    "cpi.index = pd.date_range(\"1943-01\", periods=len(cpi), freq = 'M')\n",
    "cpi = cpi['2003':]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tasa de inflacion de los ultimos 12 meses. Se usa para completar la inflacion del ultimo mes antes que se mida.\n",
    "r = cpi.pct_change().tail(12).mean()[0]\n",
    "\n",
    "## Estirar la serie de CPI hasta el dia de hoy (para precios actualizados)\n",
    "cpi = pd.concat([cpi, pd.DataFrame([], index = pd.date_range(cpi.index[-1] + 1, pd.datetime.today() + pd.DateOffset(months=1), freq = 'M'))])\n",
    "\n",
    "\n",
    "last_valid_ix = cpi.dropna().iloc[-1] # Level of the cpi index in last valid month\n",
    "\n",
    "# np.ones(len(cpi))\n",
    "cpi['avg_rate'] = pd.Series((1 + r)**range(len(cpi)), index = cpi.index)  ## Compute the exponential curve\n",
    "\n",
    "# Bring to actual level\n",
    "ratio = cpi.loc[last_valid_ix.name].avg_rate / cpi.loc[last_valid_ix.name]['index'] \n",
    "cpi['avg_rate'] = cpi['avg_rate']/ratio  \n",
    "\n",
    "# Complete cpi series up to present and remove exponential\n",
    "cpi['index'] = cpi['index'].fillna(cpi['avg_rate'])\n",
    "cpi = cpi.drop('avg_rate', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nivel_precios_actual = cpi.tail(1)['index']\n",
    "display(nivel_precios_actual)\n",
    "\n",
    "cpi = cpi.groupby(pd.TimeGrouper(freq='Q')).mean().loc[str(startyr):str(endyr)][['index']]#.to_csv(...)\n",
    "cpi.set_index('index')\n",
    "cpi.index.name = 'Q'\n",
    "#\n",
    "indice_precios = cpi\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path ='./../../../../../../home/miglesia/Documents/EPH/microdatos/' # use your path\n",
    "\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    yr = str(y)[2:]\n",
    "    allFiles = glob.glob(path + 'hogar/*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                        usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "        'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "        ['II2', 'IV5', 'IX_TOT', 'II7', 'IV4', 'II1', 'IV7', 'IV6', 'IV11', 'IV8', 'IV3', 'II8', 'IV1', 'IV10', 'II9']\n",
    "        \n",
    "        print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df = df.loc[df.IV1 != 9]\n",
    "    df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "    \n",
    "    hogar = df\n",
    "    hogar = hogar.drop_duplicates()\n",
    "    print(hogar.shape)\n",
    "\n",
    "    allFiles = glob.glob(path + 'individual/usu_individual*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        print(file_)\n",
    "    #     print(file_)\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                         usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                         ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                         'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "        df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "#         display(df.head())\n",
    "# revisar estado, condact, cat ocup, cat inac.\n",
    "    # For the regression training set. But for these the ANO4 TRIMESTRE is important.. Also we need more memory.\n",
    "    #                      ['P21','P47T',,'CH08','CH16','TOT_P12','T_VI','V10_M','V11_M','V12_M','V18_M','V19_AM','V21_M','V2_M','V3_M',\n",
    "    #             'V4_M','V5_M','V8_M','V9_M','PP08D1','PP08D4','PP08F1','PP08F2','PP08J1','PP08J2','PP08J3','PP10A','PP10C','PP10D','PP10E'])\n",
    "#         print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "    df['CH06'] = df['CH06'].clip(0)\n",
    "    df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "    df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "    \n",
    "    ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "    df['CH12'] = df.CH12.replace(99, 0)\n",
    "    df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "#     df['MAYOR'] = df['CH06'] >= 14 \n",
    "#     df['MAYOR'] = df['CH06'] // 7\n",
    "#     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "    indiv = df\n",
    "    indiv = indiv.dropna(subset = ['P47T'])\n",
    "    print(indiv.shape)\n",
    "\n",
    "    indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "    EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'], indicator = True)\n",
    "\n",
    "    print('Hogar - Indiv merged:')\n",
    "    print(EPH.shape)\n",
    "\n",
    "    \n",
    "#     EPH = EPH.loc[EPH.P47T != -9]\n",
    "    \n",
    "    EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "    EPH_no_aglo = EPH.copy(); \n",
    "    EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "    EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "    print('No aglo agregado:')\n",
    "    print(EPH.shape)\n",
    "    \n",
    "    # Quarters / deflation\n",
    "    EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "#     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "    cpi_mes_actual = nivel_precios_actual.values[0]\n",
    "    \n",
    "    EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(indice_precios, on = 'Q', how = 'left')['index'].values, 0)\n",
    "    \n",
    "    # 2018Q3 -> Mar19 1.3156\n",
    "    # 2018Q3 -> Abr19 1.361\n",
    "#     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "    \n",
    "    EPH[col_mon] = EPH[col_mon].round()\n",
    "    \n",
    "    print('deflactado:')\n",
    "    print(EPH.shape)\n",
    "    display(EPH[col_mon].mean())\n",
    "    \n",
    "    \n",
    "    training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "    training.to_csv('./../data/training/EPHARG_train_'+str(yr)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for yr in [str(s) for s in [2006, 2011, 2016]]:\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)#.sample(400000)\n",
    "    df_list += [train]\n",
    "    \n",
    "train_df = pd.concat(df_list)\n",
    "\n",
    "AGLO_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "Reg_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['Region'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'Reg_rk'})\n",
    "\n",
    "AGLO_rk['AGLO_rk'] = AGLO_rk.AGLO_rk/AGLO_rk.AGLO_rk.max()\n",
    "AGLO_rk.to_csv('./../data/AGLO_rk', index = False)\n",
    "Reg_rk['Reg_rk'] = Reg_rk.Reg_rk/Reg_rk.Reg_rk.max()\n",
    "Reg_rk.to_csv('./../data/Reg_rk', index = False)\n",
    "\n",
    "# check it out\n",
    "# AGLO_rk.merge(pd.read_csv('./../data/info/aglo_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n",
    "\n",
    "df_list = []\n",
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')#.drop(['AGLO_rk', 'Reg_rk'], axis = 1)\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)\n",
    "    train = train.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "    train.to_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set.\n",
    "\n",
    "# Older code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando censo a los DPTO elegidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.0s\n",
      "[########################################] | 100% Completed | 27.6s\n",
      "[########################################] | 100% Completed | 44.5s\n"
     ]
    }
   ],
   "source": [
    "# import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "# #Esto es para extraer las viviendas, hogares y personas de los partidos (DPTOs) en cuestion.\n",
    "\n",
    "# VIVIENDA = dd.read_csv('./VIVIENDA.csv', sep = ';', usecols = ['VIVIENDA_REF_ID', 'RADIO_REF_ID', 'TIPVV', 'V01'])\n",
    "# VIVIENDA = VIVIENDA.merge(radio_ref[['RADIO_REF_ID', 'DPTO']])\n",
    "# VIVIENDA_ = VIVIENDA.loc[VIVIENDA.DPTO.isin(seleccion_DPTOS)]\n",
    "# with ProgressBar():\n",
    "#     VIVIENDA_REF_ID_sel = VIVIENDA_['VIVIENDA_REF_ID'].values.compute()\n",
    "\n",
    "# HOGAR = dd.read_csv('./HOGAR.csv', sep = ';', usecols = ['HOGAR_REF_ID', 'VIVIENDA_REF_ID', 'H05', 'H06', 'H07', 'H08',\n",
    "#        'H09', 'H10', 'H11', 'H12', 'H13', 'H14', 'H15', 'H16', 'PROP', 'TOTPERS']) # csv is too big, so it is dask-loaded. Not sure it's efficient thou\n",
    "# # For example computing len takes ages\n",
    "# # len(HOGAR.VIVIENDA_REF_ID)\n",
    "# HOGAR_ = HOGAR.loc[HOGAR.VIVIENDA_REF_ID.isin(VIVIENDA_REF_ID_sel)]\n",
    "# with ProgressBar():\n",
    "#     HOGAR_REF_ID_sel = HOGAR_['HOGAR_REF_ID'].values.compute()\n",
    "\n",
    "# PERSONA = dd.read_csv('./PERSONA.csv', sep = ';', usecols = ['PERSONA_REF_ID', 'HOGAR_REF_ID', 'P01', 'P02', 'P03', 'P05', 'P06',\n",
    "#        'P07', 'P12', 'P08', 'P09', 'P10', 'CONDACT'])\n",
    "# PERSONA_ = PERSONA.loc[PERSONA.HOGAR_REF_ID.isin(HOGAR_REF_ID_sel)]\n",
    "\n",
    "# tabla_censo = VIVIENDA_.merge(HOGAR_)#.merge(PERSONA_)\n",
    "\n",
    "# IX_TOT = tabla_censo.groupby('HOGAR_REF_ID').count().iloc[:, 0].reset_index()\n",
    "# IX_TOT.columns = ['HOGAR_REF_ID', 'IX_TOT']\n",
    "\n",
    "# tabla_censo = tabla_censo.merge(IX_TOT)\n",
    "\n",
    "# with ProgressBar():\n",
    "#     table = tabla_censo.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making EPH and Censo answers uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Approach: modify Census to fit EPH\n",
    "# Armonizar para adecuar dataset Censo a las opciones rta de EPH. No correr 2 veces\n",
    "table['V01'] = table['V01'].map({1:1, 2:6, 3:6, 4:2, 5:3, 6:4, 7:5, 8:6})\n",
    "table['H06'] = table['H06'].map({1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:9})\n",
    "table['H09'] = table['H09'].map({1:1, 2:2, 3:3, 4:4, 5:4, 6:4})\n",
    "table['H16'] = table['H16'].clip(0, 9)\n",
    "table['H14'] = table['H14'].map({1:1, 2:4, 3:2, 4:2, 5:4, 6:3, 7:4, 8:9})\n",
    "table['H13'] = table['H13'].map({1:1, 2:2, 4:0})\n",
    "# table['P07'] = table['P07'].map({1:1, 2:2, 0:2})\n",
    "\n",
    "# saber de que aglo es la persona. Se usa los resultados de cada aglo.\n",
    "table = table.merge(radio_ref[['RADIO_REF_ID','AGLOMERADO']]) \n",
    "\n",
    "# table = table.set_index('PERSONA_REF_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only once to save time in the future\n",
    "# name =  'bolivar'#''\n",
    "# name =  'vlopez_rodriguez'#''\n",
    "name =  'rand'#''\n",
    "table.to_csv('./sample_censo_table'+str(n).zfill(3)+name+'.csv', index = False)\n",
    "\n",
    "# table = pd.read_csv('./sample_censo_table'+str(n).zfill(3)+'.csv')#.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min  2.4s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    PERSONA_ = PERSONA_.compute()\n",
    "\n",
    "table = table.merge(PERSONA_)\n",
    "\n",
    "table['P07'] = table['P07'].map({1:1, 2:2, 0:2})\n",
    "\n",
    "# Only once to save time in the future\n",
    "table.to_csv('./sample_censo_table'+str(n).zfill(3)+name+'.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34190, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIVIENDA_REF_ID</th>\n",
       "      <th>RADIO_REF_ID</th>\n",
       "      <th>TIPVV</th>\n",
       "      <th>V01</th>\n",
       "      <th>DPTO</th>\n",
       "      <th>HOGAR_REF_ID</th>\n",
       "      <th>H05</th>\n",
       "      <th>H06</th>\n",
       "      <th>H07</th>\n",
       "      <th>H08</th>\n",
       "      <th>...</th>\n",
       "      <th>P02</th>\n",
       "      <th>P03</th>\n",
       "      <th>P05</th>\n",
       "      <th>P06</th>\n",
       "      <th>P07</th>\n",
       "      <th>P12</th>\n",
       "      <th>P08</th>\n",
       "      <th>P09</th>\n",
       "      <th>P10</th>\n",
       "      <th>CONDACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4681791</td>\n",
       "      <td>13813</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6105</td>\n",
       "      <td>4307053</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19158</th>\n",
       "      <td>4691129</td>\n",
       "      <td>13853</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6105</td>\n",
       "      <td>4314259</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>4684094</td>\n",
       "      <td>13819</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6105</td>\n",
       "      <td>4308935</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22580</th>\n",
       "      <td>4692731</td>\n",
       "      <td>13861</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6105</td>\n",
       "      <td>4315487</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9618</th>\n",
       "      <td>4685920</td>\n",
       "      <td>13824</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6105</td>\n",
       "      <td>4310454</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VIVIENDA_REF_ID  RADIO_REF_ID  TIPVV  V01  DPTO  HOGAR_REF_ID  H05  \\\n",
       "142            4681791         13813      1  1.0  6105       4307053    1   \n",
       "19158          4691129         13853      2  NaN  6105       4314259    0   \n",
       "5391           4684094         13819      1  1.0  6105       4308935    1   \n",
       "22580          4692731         13861      1  1.0  6105       4315487    1   \n",
       "9618           4685920         13824      1  1.0  6105       4310454    1   \n",
       "\n",
       "       H06  H07  H08   ...     P02  P03  P05  P06  P07  P12  P08  P09  P10  \\\n",
       "142    4.0    1    1   ...       1   57    1    0    1    2    2    2    1   \n",
       "19158  NaN    0    0   ...       1   48    2  221    1    0    2    2    1   \n",
       "5391   4.0    1    1   ...       1   47    1    0    1    2    2    2    1   \n",
       "22580  4.0    1    1   ...       1   60    1    0    1    2    2    2    1   \n",
       "9618   1.0    1    1   ...       1   62    1    0    1    2    2    4    2   \n",
       "\n",
       "       CONDACT  \n",
       "142          1  \n",
       "19158        0  \n",
       "5391         1  \n",
       "22580        1  \n",
       "9618         1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AGLOS_censo = \n",
    "radio_ref_sel = radio_ref.loc[table['RADIO_REF_ID'].drop_duplicates().values]\n",
    "aglos_sel = radio_ref_sel.AGLOMERADO.unique()\n",
    "\n",
    "## Entradas de censo de los DPTOs elegidos\n",
    "\n",
    "print(table.shape) #cuanta (gente, variables)?\n",
    "table.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adoptar mismos nombres de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Misma info, distinto nombre. \n",
    "# Censo INDEC \n",
    "md_1 = table[['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO', #las que no se erran, cant pers, sexo, edad, act, aglo\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']] #las x que buscan matches un poquito mas laxamente\n",
    "\n",
    "\n",
    "#Mismas cosas, distinto nombre de columna para\n",
    "# EPH INDEC\n",
    "md_2 = EPH[['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']]\n",
    "\n",
    "# # Now we want to see in each column what are the percentages, as a clue to where there can be issues\n",
    "# # OK control check. Control there is less likely confusion. \n",
    "\n",
    "# for i in range(len(md_1.columns))[:2]: \n",
    "#     print('\\n')\n",
    "#     for md in [md_1, md_2]:\n",
    "#         col = md.columns[i]\n",
    "#         print(col)\n",
    "#         print(md[col].value_counts().sort_index()/len(md))\n",
    "\n",
    "md_2.columns = md_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### The 'y' variables will be predicted. K nearest neighbors is used.\n",
    "## Variables in the EPH survey but not in the Censo.\n",
    "## Preguntas de EncuestaPH que no estan en Censo.\n",
    "\n",
    "y_cols = ['P21','P47T','CAT_INAC','CAT_OCUP','CH07','CH08','CH16','TOT_P12','T_VI','V10_M','V11_M','V12_M','V18_M','V19_AM','V21_M','V2_M','V3_M','V4_M','V5_M','V8_M','V9_M',\n",
    "         'PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59','PP07H','PP07I','PP07J','PP07K','PP08D1','PP08D4','PP08F1','PP08F2','PP08J1','PP08J2','PP08J3','PP10A','PP10C','PP10D','PP10E']\n",
    "\n",
    "#Remove ill predicted ones after trying them out\n",
    "y_cols = list(set(y_cols) - set(['PP10A', 'V11_M', 'PP08D4', 'PP08J3', 'PP08F1', 'V18_M', 'V10_M',\n",
    "       'V8_M', 'V4_M', 'PP08F2', 'V21_M', 'V9_M', 'PP08J2', 'PP08J1',\n",
    "       'V19_AM']))\n",
    "\n",
    "\n",
    "\n",
    "train = md_2.join(EPH[y_cols]).fillna(0)\n",
    "test = md_1.fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO', 'V01', 'H05', 'H06',\n",
       "       'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14',\n",
       "       'H13', 'P07', 'P08', 'P09', 'P10', 'P05'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('./train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58977"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58977, 50)\n",
      "(34190, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7249780637613337"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se entrena en .8% de la poblacion... con solo tener La Matanza y La Plata. Parece poco: error prone.\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "len(train)/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Las variables principales que no se erran. Personas en hogar, su sexo, su decil de edad (~ grupo 6 anios) y condicion de actividad, o sea si sale a trabajar o 'esta en casa'.\n",
    "\n",
    "# train[['IX_TOT', 'P02', 'P03', 'CONDACT']].nunique().values\n",
    "# train[['IX_TOT', 'P02', 'P03', 'CONDACT']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6068441064638783"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[x_cols].drop_duplicates())/len(test[x_cols])\n",
    "# test[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The columns that we will be making predictions with.\n",
    "x_cols = md_1.columns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Create the knn model.\n",
    "# Look at the five closest neighbors.\n",
    "knn = KNeighborsRegressor(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the training data.\n",
    "train = train.sample(frac = 1)\n",
    "#Add noise to reduce tie situations in a more or less random way\n",
    "knn.fit(train[x_cols] + .1*(np.random.random_sample(train[x_cols].shape)-0.5), train[y_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# Make point predictions on the test set using the fit model.\n",
    "predictions = knn.predict(test[x_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions, columns=y_cols, index = test.index)\n",
    "\n",
    "result = test.join(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1763261, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IX_TOT</th>\n",
       "      <th>P02</th>\n",
       "      <th>P03</th>\n",
       "      <th>CONDACT</th>\n",
       "      <th>AGLOMERADO</th>\n",
       "      <th>V01</th>\n",
       "      <th>H05</th>\n",
       "      <th>H06</th>\n",
       "      <th>H07</th>\n",
       "      <th>H08</th>\n",
       "      <th>H09</th>\n",
       "      <th>H10</th>\n",
       "      <th>H11</th>\n",
       "      <th>H12</th>\n",
       "      <th>H16</th>\n",
       "      <th>H15</th>\n",
       "      <th>PROP</th>\n",
       "      <th>H14</th>\n",
       "      <th>H13</th>\n",
       "      <th>P07</th>\n",
       "      <th>P08</th>\n",
       "      <th>P09</th>\n",
       "      <th>P10</th>\n",
       "      <th>P05</th>\n",
       "      <th>V12_M</th>\n",
       "      <th>PP08D1</th>\n",
       "      <th>PP07K</th>\n",
       "      <th>PP07G4</th>\n",
       "      <th>V2_M</th>\n",
       "      <th>CAT_OCUP</th>\n",
       "      <th>V3_M</th>\n",
       "      <th>TOT_P12</th>\n",
       "      <th>PP07G2</th>\n",
       "      <th>PP10C</th>\n",
       "      <th>T_VI</th>\n",
       "      <th>CH16</th>\n",
       "      <th>CH07</th>\n",
       "      <th>PP07H</th>\n",
       "      <th>PP10E</th>\n",
       "      <th>PP07G3</th>\n",
       "      <th>PP07J</th>\n",
       "      <th>PP07G_59</th>\n",
       "      <th>PP07I</th>\n",
       "      <th>PP10D</th>\n",
       "      <th>CH08</th>\n",
       "      <th>PP07G1</th>\n",
       "      <th>CAT_INAC</th>\n",
       "      <th>P47T</th>\n",
       "      <th>P21</th>\n",
       "      <th>V5_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7345.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7345.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IX_TOT  P02  P03  CONDACT  AGLOMERADO  V01  H05  H06  H07  H08  H09  H10  \\\n",
       "0       3    1    8      100           0    1    1    4    1    1    2    1   \n",
       "1       3    2    7      300           0    1    1    4    1    1    2    1   \n",
       "2       3    1    2      300           0    1    1    4    1    1    2    1   \n",
       "3       4    1    8      100           0    1    1    4    1    1    2    1   \n",
       "4       4    1    3      100           0    1    1    4    1    1    2    1   \n",
       "\n",
       "   H11  H12  H16  H15  PROP  H14  H13  P07  P08  P09  P10  P05  V12_M  \\\n",
       "0    1    2    3    2     1    2    1    1    2    4    2    1    0.0   \n",
       "1    1    2    3    2     1    2    1    1    2    7    2    1    0.0   \n",
       "2    1    2    3    2     1    2    1    1    1    4    2    1    0.0   \n",
       "3    1    2    5    3     4    2    1    1    2    2    1    1    0.0   \n",
       "4    1    2    5    3     4    2    1    1    2    5    1    1    0.0   \n",
       "\n",
       "    PP08D1  PP07K  PP07G4    V2_M  CAT_OCUP  V3_M  TOT_P12  PP07G2  PP10C  \\\n",
       "0  30000.0    1.0     1.0     0.0       3.0   0.0      0.0     1.0    0.0   \n",
       "1      0.0    0.0     0.0  7200.0       0.0   0.0      0.0     0.0    0.0   \n",
       "2      0.0    0.0     0.0     0.0       0.0   0.0      0.0     0.0    0.0   \n",
       "3     -9.0    1.0     1.0     0.0       3.0   0.0      0.0     1.0    0.0   \n",
       "4      0.0    0.0     0.0     0.0       2.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "     T_VI  CH16  CH07  PP07H  PP10E  PP07G3  PP07J  PP07G_59  PP07I  PP10D  \\\n",
       "0     0.0   1.0   2.0    1.0    0.0     1.0    1.0       0.0    0.0    0.0   \n",
       "1  7345.0   1.0   4.0    0.0    0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "2     0.0   1.0   5.0    0.0    0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "3     0.0   1.0   2.0    1.0    0.0     1.0    3.0       0.0    0.0    0.0   \n",
       "4     0.0   1.0   5.0    0.0    0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "\n",
       "   CH08  PP07G1  CAT_INAC     P47T      P21   V5_M  \n",
       "0   1.0     1.0       0.0  30000.0  30000.0    0.0  \n",
       "1   1.0     0.0       1.0   7345.0      0.0  145.0  \n",
       "2   4.0     0.0       3.0      0.0      0.0    0.0  \n",
       "3   1.0     1.0       0.0     -9.0     -9.0    0.0  \n",
       "4   4.0     0.0       0.0   4000.0   4000.0    0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 990\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "print(result.shape)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.to_csv('./test_result_'+str(n).zfill(3)+'dpto_1.csv')\n",
    "# result.to_csv('./test_result_'+str(n).zfill(3)+'dpto.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_1 = pd.read_csv('./test_result_'+str(n).zfill(3)+'dpto_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# res_0 = pd.read_csv('./test_result_'+str(n).zfill(3)+'dpto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-423399ca1037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# porcentaje error por reshuffle:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mres_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mres_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# perr_.tail(15)#.index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Bad ones: ['PP10A', 'V11_M', 'PP08D4', 'PP08J3', 'PP08F1', 'V18_M', 'V10_M',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_0' is not defined"
     ]
    }
   ],
   "source": [
    "# porcentaje error por reshuffle: \n",
    "perr_ = np.round(100*abs(res_0.sum() - res_1.sum())/res_0.sum(), 1).sort_values()\n",
    "\n",
    "# perr_.tail(15)#.index\n",
    "# Bad ones: ['PP10A', 'V11_M', 'PP08D4', 'PP08J3', 'PP08F1', 'V18_M', 'V10_M',\n",
    "#        'V8_M', 'V4_M', 'PP08F2', 'V21_M', 'V9_M', 'PP08J2', 'PP08J1',\n",
    "#        'V19_AM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Agregar $$$. En millones de usd\n",
    "# En millones de usd (USD = 30 ARS)\n",
    "_USD = 30.5 #ARS\n",
    "np.round(res_1.sum()/_USD/1e6, 1).sort_values().tail(14)\n",
    "\n",
    "#PPALES\n",
    "# negocio que no trabajo no laborable (V9_M)\n",
    "# alquiler no laborable (V8_M)\n",
    "# indemnizacion despido no laborable (V3_M)\n",
    "# comision Ocupacion ppal (PP08F1)\n",
    "# cuota alimentos no laborable (V12_M)\n",
    "# subsidio ayuda social no laborable (V5_M)\n",
    "# TOTAL otras ocupacions(TOT_P12)\n",
    "# jubilacion no laborable (V2_M)\n",
    "# TOTAL no laborables (T_VI)\n",
    "# sueldo Ocupacion ppal(PP08D1)\n",
    "# TOTAL Ocupacion ppal (P21)\n",
    "# TOTAL TOTAL (P47T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.round(res_1.sum()/1e6/_USD, 1).sort_values().tail(14).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PERS_DPTO = table[['PERSONA_REF_ID', 'RADIO_REF_ID']].merge(radio_ref_sel[['RADIO_REF_ID', 'DPTO' #, 'NOMDPTO', 'NOMPROV'\n",
    "                                                                      ]]).drop(['RADIO_REF_ID'], axis = 1)\n",
    "\n",
    "res = res_1\n",
    "res_DPTO = res.merge(PERS_DPTO, on = 'PERSONA_REF_ID')\n",
    "\n",
    "#en miles de USD\n",
    "res_byDPTO = res_DPTO.groupby(['DPTO'])[[  'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "       'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']].sum()\n",
    "\n",
    "np.round(100*res_byDPTO.div(res_byDPTO.P47T, axis = 0), 1).sort_values(by = 'P21').head() #percentage\n",
    "# np.round(res_byDPTO/1e3/_USD, 1) #in 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PERS_DPTO = table[['PERSONA_REF_ID', 'RADIO_REF_ID']].merge(radio_ref_sel[['RADIO_REF_ID', 'DPTO', 'NOMDPTO', 'NOMPROV']]\n",
    "                                                           )#.drop(['RADIO_REF_ID'], axis = 1)\n",
    "\n",
    "res = res_1\n",
    "res_DPTO = res.merge(PERS_DPTO, on = 'PERSONA_REF_ID')\n",
    "\n",
    "# variables = ['V9_M', 'V8_M', 'PP08F1', 'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "#        'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']\n",
    "variables = [  'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "       'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']\n",
    "#en miles de USD\n",
    "# res_byDPTO = res_DPTO.groupby(['DPTO', 'NOMDPTO', 'NOMPROV'])[variables].sum()\n",
    "res_byDPTO = res_DPTO.groupby(['RADIO_REF_ID'])[variables].sum()\n",
    "\n",
    "# np.round(100*res_byDPTO.div(res_byDPTO.P47T, axis = 0), 1).sort_values(by = 'P21').head() #percentage\n",
    "np.round(res_byDPTO/1e3/_USD, 1) #in 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save info at 'radio' level\n",
    "res_byDPTO.to_csv('res_byradio_sample_'+str(n).zfill(3)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Desoc, NA = 0. Not good.\n",
    "variables = ['PP07J', #turno habitual\n",
    " 'PP10D', #Desoc. Ha trabajado alguna vez?\n",
    " 'PP10C', #Desoc. Hizo changa mientras buscaba?\n",
    " 'PP07K', # Oc. ppal. Inc. serv. dom. Cobra con recibo\n",
    " 'PP07G2', # Oc. ppal. Inc. serv. dom. aguinaldo\n",
    " 'PP07G_59', # Oc. ppal. Inc. serv. dom. ninguno\n",
    " 'PP07G3', # Oc. ppal. Inc. serv. dom. dias enfermedad\n",
    " 'PP10E', # Desoc. Tiempo de que termino su ultimo trabajo/changa\n",
    " 'PP07H', # Oc. ppal. Inc. serv. dom. descuento jubilatorio\n",
    " 'PP07G4', # Oc. ppal. Inc. serv. dom. obra social\n",
    " 'PP07I', # Oc. ppal. Inc. serv. dom. Aporta jub por sí mismo \n",
    " 'PP07G1', # Oc. ppal. Inc. serv. dom. vacaciones pagas\n",
    " 'CH07'] #Est civil\n",
    "\n",
    "#en miles de USD\n",
    "# res_byDPTO = res_DPTO.groupby(['DPTO', 'NOMDPTO', 'NOMPROV'])[variables].mean()\n",
    "res_byDPTO = res_DPTO.groupby(['RADIO_REF_ID'])[variables].mean()\n",
    "\n",
    "\n",
    "s = np.round(res_byDPTO, 2).sort_values(by = 'PP07K')#.head() \n",
    "s.style.bar(color='#d65f5f')\n",
    "#  'CAT_OCUP', #CAT_INAC\n",
    "#  'CAT_INAC', #CAT_INAC\n",
    "\n",
    "#  'CH08', obra social/salud. nums altos como para mean\n",
    "# 'CH16', # Donde vivia hace 5. Desconfiable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# neigh = NearestNeighbors(n_neighbors=1)\n",
    "# neigh.fit(train[x_cols], train.sample(frac = 1)[y_cols])\n",
    "# i = 60\n",
    "# print(neigh.kneighbors([test.iloc[i].values], return_distance=True))\n",
    "# train.iloc[7022][x_cols] - test.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in result.columns:\n",
    "    print('\\n')\n",
    "    print(col)\n",
    "    df_ = result.loc[result.P03 > 2]\n",
    "    print(df_[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
