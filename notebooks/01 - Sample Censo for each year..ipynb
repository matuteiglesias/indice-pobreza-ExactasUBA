{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On this notebook we extract Censo 2010 individual data from their files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los numeros de la tabla proyeccion de poblacion discrepan del censo 2010.\n",
    "# Para solucionar, voy a usar los ratios que propone INDEC post 2010, pero sobreescribiendo el valor de 2010 con lo que dice el censo.\n",
    "proy_pop = pd.read_csv('./../data/info/proy_pop200125.csv', encoding = 'utf-8')\n",
    "\n",
    "# poblacion 2010\n",
    "P02 = pd.read_csv('./../data/info/PERSONA-P02.csv', encoding = 'latin-1')\n",
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "radio_ref['DPTO'] = radio_ref['DPTO'].astype(int)\n",
    "P02_geo = P02.merge(radio_ref[['DPTO', 'NOMDPTO','PROV','NOMPROV', 'radio']])\n",
    "pob2010_DPTO = P02_geo.groupby(['PROV','NOMPROV','DPTO','NOMDPTO'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "merged = proy_pop.merge(pob2010_DPTO, left_on='Cï¿½digo', right_on = 'DPTO') \n",
    "# solo se pierde la antartida\n",
    "merged[[str(s) for s in range(2010, 2026)]] = round(merged[[str(s) for s in range(2010, 2026)]].T*merged.TOTAL/merged['2010']).astype(int).T\n",
    "\n",
    "proy_pob = merged.set_index(['PROV','NOMPROV','DPTO','NOMDPTO'])[[str(s) for s in range(2001, 2026)]]\n",
    "\n",
    "#redo linear interpolation of 2001 -> 2010\n",
    "proy_pob[[str(s) for s in range(2002, 2010)]] = np.nan\n",
    "proy_pob = round(proy_pob.interpolate(axis = 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacen Falta los datasets PERSONA, VIVIENDA, HOGAR. Se extraen de la base de datos del Censo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular columna IX_TOT (n. personas en hogar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/BCNCPC23/Desktop/extracted_/PERSONA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-53c16317bb9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Commented after it could be saved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Cantidad de personas  en cada hogar. No esta la columna pero la podemos construir..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mPERSONA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./../../extracted_/PERSONA.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'HOGAR_REF_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# csv is too big, so it is dask-loaded. Not sure it's efficient thou\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\csv.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     ):\n\u001b[1;32m--> 645\u001b[1;33m         return read_pandas(\n\u001b[0m\u001b[0;32m    646\u001b[0m             \u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[0murlpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[1;34m(reader, urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[0mb_lineterminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m     b_out = read_bytes(\n\u001b[0m\u001b[0;32m    480\u001b[0m         \u001b[0murlpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_lineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\bytes\\core.py\u001b[0m in \u001b[0;36mread_bytes\u001b[1;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m                     \u001b[1;34m\"To read, set blocksize=None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 )\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"size\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fsspec\\implementations\\local.py\u001b[0m in \u001b[0;36minfo\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strip_protocol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mdest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/BCNCPC23/Desktop/extracted_/PERSONA.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Commented after it could be saved\n",
    "# Cantidad de personas  en cada hogar. No esta la columna pero la podemos construir..\n",
    "PERSONA = dd.read_csv('./../../extracted_/PERSONA.csv', sep = ';', usecols = ['HOGAR_REF_ID']) # csv is too big, so it is dask-loaded. Not sure it's efficient thou\n",
    "\n",
    "with ProgressBar():\n",
    "    IX_TOT = PERSONA['HOGAR_REF_ID'].value_counts().reset_index().compute()\n",
    "    IX_TOT.columns = ['HOGAR_REF_ID', 'IX_TOT']\n",
    "    \n",
    "IX_TOT.to_csv('./../data/info/IX_TOT.csv')\n",
    "\n",
    "IX_TOT = pd.read_csv('./../data/info/IX_TOT.csv')\n",
    "IX_TOT.head() # el tamanio de 12.197.647 hogares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radio_ref = pd.read_csv('./../Censo_individual/data/radio_ref.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hogar y Vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.02\n",
    "\n",
    "#Esto es para extraer las viviendas, hogares y personas de los partidos (DPTOs) en cuestion.\n",
    "# seleccion_DPTOS y usecols nos sirven para no cargar data innecesaria.\n",
    "VIVIENDA = dd.read_csv('./../../extracted_/VIVIENDA.csv', sep = ';', usecols = ['VIVIENDA_REF_ID', 'RADIO_REF_ID', 'TIPVV', 'V01'])\n",
    "VIVIENDA = VIVIENDA.merge(radio_ref[['RADIO_REF_ID', 'DPTO']])\n",
    "\n",
    "\n",
    "# VIVIENDA_ = VIVIENDA.loc[VIVIENDA.DPTO.isin(seleccion_DPTOS)]\n",
    "    \n",
    "HOGAR = dd.read_csv('./../../extracted_/HOGAR.csv', sep = ';', usecols = ['HOGAR_REF_ID', 'VIVIENDA_REF_ID']) # csv is too big, so it is dask-loaded. Not sure it's efficient thou\n",
    "\n",
    "with ProgressBar():\n",
    "    HOGAR_DPTO = HOGAR.merge(VIVIENDA[['VIVIENDA_REF_ID', 'DPTO']]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2020\n",
    "endyr = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grouped = HOGAR_DPTO.merge(proyeccion[['DPTO', 'ratio_18']]).groupby('DPTO')\n",
    "ratios = proy_pob.div(proy_pob['2010'], 0).reset_index()\n",
    "\n",
    "\n",
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    grouped = HOGAR_DPTO.merge(ratios[['DPTO', yr]]).groupby('DPTO')\n",
    "    sample = grouped.apply(lambda x: x.sample(frac=frac*x[yr].mean()))\n",
    "\n",
    "    HOGAR = dd.read_csv('./../../extracted_/HOGAR.csv', sep = ';', usecols = ['HOGAR_REF_ID', 'VIVIENDA_REF_ID', 'H05', 'H06', 'H07', 'H08',\n",
    "           'H09', 'H10', 'H11', 'H12', 'H13', 'H14', 'H15', 'H16', 'PROP', 'TOTPERS']) \n",
    "\n",
    "    VIVIENDA_sample = VIVIENDA.loc[VIVIENDA.VIVIENDA_REF_ID.isin(sample.VIVIENDA_REF_ID)]\n",
    "    HOGAR_sample = HOGAR.loc[HOGAR.HOGAR_REF_ID.isin(sample.HOGAR_REF_ID)]\n",
    "\n",
    "    tabla_censo = VIVIENDA_sample.merge(HOGAR_sample)\n",
    "    tabla_censo = tabla_censo.merge(IX_TOT)\n",
    "\n",
    "    with ProgressBar():\n",
    "        table = tabla_censo.compute()\n",
    "\n",
    "\n",
    "    # Approach: modify Census to fit EPH\n",
    "    table['V01'] = table['V01'].map({1:1, 2:6, 3:6, 4:2, 5:3, 6:4, 7:5, 8:6})\n",
    "    table['H06'] = table['H06'].map({1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:9})\n",
    "    table['H09'] = table['H09'].map({1:1, 2:2, 3:3, 4:4, 5:4, 6:4})\n",
    "    table['H16'] = table['H16'].clip(0, 9)\n",
    "    table['H14'] = table['H14'].map({1:1, 2:4, 3:2, 4:2, 5:4, 6:3, 7:4, 8:9})\n",
    "    table['H13'] = table['H13'].map({1:1, 2:2, 4:0})\n",
    "\n",
    "    # saber de que aglo es la persona. Se usa los resultados de cada aglo.\n",
    "    table = table.merge(radio_ref[['RADIO_REF_ID','AGLOMERADO']]) \n",
    "\n",
    "#     Only once to save time in the future.\n",
    "\n",
    "    table.to_csv('./../../extracted_/yr_samples/sample_censo_table_f'+str(frac)+'_'+yr+'_ARG.csv', index = False)\n",
    "\n",
    "    PERSONA = dd.read_csv('./../../extracted_/PERSONA.csv', sep = ';', usecols = ['PERSONA_REF_ID', 'HOGAR_REF_ID', 'P01', 'P02', 'P03', 'P05', 'P06',\n",
    "           'P07', 'P12', 'P08', 'P09', 'P10', 'CONDACT'])\n",
    "\n",
    "    PERSONA_sample = PERSONA.loc[PERSONA.HOGAR_REF_ID.isin(sample.HOGAR_REF_ID)]\n",
    "\n",
    "\n",
    "    with ProgressBar():\n",
    "        table = table.merge(PERSONA_sample.compute())\n",
    "\n",
    "    table['P07'] = table['P07'].map({1:1, 2:2, 0:2})\n",
    "\n",
    "    df = table[['RADIO_REF_ID']].merge(radio_ref, on = 'RADIO_REF_ID', how = 'left')\n",
    "    display(df[['IDPROV','PROV']].nunique())\n",
    "\n",
    "    # Only once to save time in the future\n",
    "    table.to_csv('./../../extracted_/yr_samples/sample_censo_table_f'+str(frac)+'_'+yr+'_ARG.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
